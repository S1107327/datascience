{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037a4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27144ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lettura dati ed embedding\n",
    "#ipotizzo var \"dataset\" {data, label}\n",
    "\n",
    "#random state per test con lo stesso split ogni volta, poi si può togliere\n",
    "train, test, label_train, label_test=train_test_split(dataset[\"data\"], dataset[\"label\"], test_size=0.2, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep classificatori\n",
    "names=[\"SVC\", \"KNN\", \"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"Gradient Boosting\", \"MLP\"]\n",
    "classifiers=[ SVC(), KNeighborsClassifier(n_neighbors=5), GaussianNB(), RandomForestClassifier(n_estimators=10),\n",
    "            AdaBoostClassifier(n_estimator=10), GradientBoostingClassifier(n_estimator=100), MLPClassifier()]\n",
    "\n",
    "\"\"\"\n",
    "POSSIBILI PARAM DA GESTIRE SECONDO ME\n",
    "\n",
    "KNN\n",
    "default k=5\n",
    "\n",
    "ALBERO\n",
    "criterion{“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "max_depth int, default=None\n",
    "max_features int, float or {“auto”, “sqrt”, “log2”}, default=None\n",
    "    (The number of features to consider when looking for the best split)\n",
    "min_samples_split int or float, default=2\n",
    "    (The minimum number of samples required to split an internal node)\n",
    "\n",
    "Per avere sempre gli stessi risultati:\n",
    "random_state int, RandomState instance or None, default=None \n",
    "    (The features are always randomly permuted at each split, even if splitter is set to \"best\")\n",
    "    \n",
    "RANDOM FOREST\n",
    "n_estimators int, default=100\n",
    "criterion{“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "max_depth int, default=None\n",
    "max_features{“sqrt”, “log2”, None}, int or float, default=”sqrt”. Funzione del numero di feature.\n",
    "    (The number of features to consider when looking for the best split)\n",
    "min_samples_split int or float, default=2\n",
    "    (The minimum number of samples required to split an internal node)\n",
    "    \n",
    "Per avere sempre gli stessi risultati uguale agli alber\n",
    "\n",
    "ADABOOST\n",
    "default stimatori è 50\n",
    "learning_rate: float, default=1.0\n",
    "\n",
    "GRADIENT BOOSTING\n",
    "default stimatori (alberi) è 100\n",
    "learning rate default=0.1\n",
    "parametri analoghi agli alberi\n",
    "\n",
    "MLP\n",
    "hidden_layer_sizes\n",
    "solver{‘lbfgs’, ‘sgd’, ‘adam’}, default=’adam’ (dicono che lbfgs tende ad essere meglio con dataset piccoli, non so\n",
    "come conta il nostro)\n",
    "batch_size:int, default=’auto’\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train e test dei classificatori\n",
    "\n",
    "for name, classif in zip(names, classifiers):\n",
    "    #Negli esempi di sklearn in cui confronta i classificatori usa standardscaler per standardizzare le feature\n",
    "    #non so dire se serve, per ora faccio come là\n",
    "    '''Dalla doc di StandardScaler(): Standardize features by removing the mean and scaling to unit variance.\n",
    "#Standardization of a dataset is a common requirement for many machine learning estimators: \n",
    "they might behave badly if the individual features do not more or less look like standard normally distributed data'''\n",
    "    \n",
    "    classif=make_pipeline(StandardScaler(), classif)\n",
    "    classif.fit(train, label_train)\n",
    "    print(f\"{mame}. Accuracy: {classif.score(test, label_test)}\")\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(\n",
    "        classif,\n",
    "        test,\n",
    "        label_test,\n",
    "        display_labels=[\"legit\", \"dga\"]\n",
    "        cmap=plt.cm.Blues,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    disp.ax_.set_title(f\"{name}. Confusion matrix\")\n",
    "\n",
    "    print(disp.confusion_matrix)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "names=[\"K-means\", \"DBSCAN\"]\n",
    "cluster_alg=[KMeans(n_clusters=2), DBSCAN(eps=3, min_samples=100)] #param del dbscan?\n",
    "\n",
    "for name, cluster in zip(names, cluster_alg):\n",
    "    cluster.fit(train)\n",
    "    #come mostriamo i risultati? A mano come BDA?\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
